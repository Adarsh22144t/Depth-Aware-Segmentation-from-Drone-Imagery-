{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b8e2e-36d2-4657-a773-0c8ece06cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Train samples: 1260, Val samples: 140\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /Users/sadik2/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 161M/161M [02:28<00:00, 1.14MB/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- USER SETTINGS ----------------\n",
    "TRAIN_DIR = \"/Users/sadik2/main_project/train\"\n",
    "IMG_SIZE = 256  # reduced for better naive accuracy\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "VAL_SPLIT = 0.1\n",
    "BASE_FILTERS = 32  # slightly larger fusion network\n",
    "SAVE_DIR = \"checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "# -----------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------- DATASET ----------\n",
    "class RGBDepthSegDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size=(256,256)):\n",
    "        self.image_size = image_size\n",
    "        self.rgb_paths, self.depth_paths, self.seg_paths = [], [], []\n",
    "\n",
    "        for category in os.listdir(root_dir):\n",
    "            cat_path = os.path.join(root_dir, category)\n",
    "            if not os.path.isdir(cat_path): continue\n",
    "            for folder in os.listdir(cat_path):\n",
    "                folder_path = os.path.join(cat_path, folder)\n",
    "                image_folder = os.path.join(folder_path, \"image\")\n",
    "                depth_folder = os.path.join(folder_path, \"depth\")\n",
    "                seg_folder = os.path.join(folder_path, \"segmentation\")\n",
    "                if not (os.path.exists(image_folder) and os.path.exists(depth_folder) and os.path.exists(seg_folder)):\n",
    "                    continue\n",
    "                files = sorted([f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))])\n",
    "                for f in files:\n",
    "                    self.rgb_paths.append(os.path.join(image_folder, f))\n",
    "                    self.depth_paths.append(os.path.join(depth_folder, f))\n",
    "                    self.seg_paths.append(os.path.join(seg_folder, f))\n",
    "\n",
    "        self.transform_rgb = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.transform_depth = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.transform_seg = transforms.Compose([\n",
    "            transforms.Resize(self.image_size, interpolation=Image.NEAREST)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb = Image.open(self.rgb_paths[idx]).convert(\"RGB\")\n",
    "        depth = Image.open(self.depth_paths[idx]).convert(\"L\")\n",
    "        seg = Image.open(self.seg_paths[idx])\n",
    "        rgb = self.transform_rgb(rgb)\n",
    "        depth = self.transform_depth(depth)\n",
    "        seg = self.transform_seg(seg)\n",
    "        seg = torch.as_tensor(np.array(seg), dtype=torch.long)\n",
    "        return {\"rgb\": rgb, \"depth\": depth, \"gt_seg\": seg}\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "dataset = RGBDepthSegDataset(TRAIN_DIR, image_size=(IMG_SIZE, IMG_SIZE))\n",
    "val_size = int(len(dataset) * VAL_SPLIT)\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n",
    "\n",
    "# ---------- MODELS ----------\n",
    "import torchvision\n",
    "\n",
    "# Use a stronger backbone for better naive accuracy\n",
    "def load_seg_backbone(device):\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet50(\n",
    "        weights=\"DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\"\n",
    "    ).to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "seg_backbone = load_seg_backbone(device)\n",
    "\n",
    "# ---------- FUSION NETWORK ----------\n",
    "class TinyFuseNet(nn.Module):\n",
    "    def __init__(self, in_seg_channels, num_classes, base=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_seg_channels + 1, base, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(base, base, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(base, num_classes, 1)\n",
    "\n",
    "    def forward(self, seg_logits, depth_map):\n",
    "        x = torch.cat([seg_logits, depth_map], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# Infer num_classes\n",
    "sample_seg = dataset[0][\"gt_seg\"].numpy()\n",
    "num_classes = int(sample_seg.max()) + 1\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "    seg_ch = seg_backbone(dummy)[\"out\"].shape[1]\n",
    "\n",
    "fusion = TinyFuseNet(seg_ch, num_classes=num_classes, base=BASE_FILTERS).to(device)\n",
    "optimizer = torch.optim.Adam(fusion.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ---------- HELPER FUNCTIONS ----------\n",
    "def compute_pixel_accuracy(pred, target):\n",
    "    correct = (pred == target).float()\n",
    "    return correct.sum() / correct.numel()\n",
    "\n",
    "def get_seg_logits(batch_rgb):\n",
    "    with torch.no_grad():\n",
    "        logits = seg_backbone(batch_rgb)[\"out\"]\n",
    "    if logits.shape[2:] != (IMG_SIZE, IMG_SIZE):\n",
    "        logits = F.interpolate(logits, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "    return logits\n",
    "\n",
    "# ---------- TRAINING LOOP ----------\n",
    "for epoch in range(EPOCHS):\n",
    "    fusion.train()\n",
    "    running_loss = 0.0\n",
    "    naive_acc_sum = 0.0\n",
    "    fusion_acc_sum = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        rgb = batch[\"rgb\"].to(device)\n",
    "        depth = batch[\"depth\"].to(device)\n",
    "        gt_seg = batch[\"gt_seg\"].to(device)\n",
    "\n",
    "        naive_logits = get_seg_logits(rgb)\n",
    "        naive_pred = naive_logits.argmax(1)\n",
    "        naive_acc = compute_pixel_accuracy(naive_pred, gt_seg)\n",
    "\n",
    "        # --- Fusion ---\n",
    "        fused_logits = fusion(naive_logits, depth)\n",
    "        fused_pred = fused_logits.argmax(1)\n",
    "        fusion_acc = compute_pixel_accuracy(fused_pred, gt_seg)\n",
    "\n",
    "        # Prepare target\n",
    "        gt_seg_scaled = (gt_seg / gt_seg.max() * (seg_ch - 1)).long()\n",
    "        loss = criterion(fused_logits, gt_seg_scaled)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        naive_acc_sum += naive_acc.item()\n",
    "        fusion_acc_sum += fusion_acc.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = running_loss / total_batches\n",
    "    avg_naive_acc = naive_acc_sum / total_batches\n",
    "    avg_fusion_acc = fusion_acc_sum / total_batches\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Loss: {avg_loss:.4f} | Naive Acc: {avg_naive_acc:.4f} | Fusion Acc: {avg_fusion_acc:.4f}\")\n",
    "\n",
    "    # Save model weights after every epoch\n",
    "    torch.save(fusion.state_dict(), os.path.join(SAVE_DIR, f\"fusion_epoch{epoch+1}.pth\"))\n",
    "\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b17c7a-8bfb-4135-bd45-ff62f3dae478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d941dc-283e-4e27-b316-46ae3cfa57de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40caeaec-16ab-4d8e-ad33-7611db9a0bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
